{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3618120-e5d7-4d97-9c3f-14c12ea72064",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values. <br>\n",
    "Missing values in a dataset refer to the absence of a value for a particular variable or feature in some of the data points. The reasons for missing values can vary, including data entry errors, equipment failure, and non-response in surveys. Handling missing values is essential because it can affect the quality of the analysis, and may lead to biased results or incorrect conclusions.\n",
    "\n",
    "There are several ways to handle missing values, including removing the data points with missing values, imputing the missing values with a suitable value (e.g., mean, median, or mode), or using advanced techniques such as multiple imputation or machine learning-based imputation.\n",
    "\n",
    "Some algorithms that are not affected by missing values include decision trees, random forests, and K-nearest neighbors (KNN) because they can handle missing values in a natural way by considering only the available features for each data point. Other algorithms such as linear regression, logistic regression, and neural networks may require imputation or removal of missing values before fitting the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5376c-44cf-4e86-bffd-90c6fa12c37e",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.<br>\n",
    "Here are some techniques used to handle missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fad5bcc-66bc-4097-b5d3-b94609e9214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index    A    B   C\n",
      "0      0  1.0  5.0   9\n",
      "1      3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Dropping missing values: \n",
    "In this method, the rows or columns with missing values are dropped from the dataset. \n",
    "This method is useful when the amount of missing data is small.\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Drop the rows with missing values\n",
    "df_dropped = df.dropna().reset_index()\n",
    "print(df_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288fed82-0b7b-4001-841f-6ccf7920f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B   C\n",
      "0  1.000000  5.000000   9\n",
      "1  2.000000  6.666667  10\n",
      "2  2.333333  7.000000  11\n",
      "3  4.000000  8.000000  12\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Imputation: In this method, the missing values are filled with substitute values,\n",
    "such as mean, median, or mode of the non-missing values.\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Fill the missing values with mean\n",
    "df_imputed = df.fillna(df.mean())\n",
    "print(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58688051-b91c-4d28-8f49-7f1e20f924aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "1  2.0  5.0  10\n",
      "2  2.0  7.0  11\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Forward filling: In this method, the missing values are filled with the previous non-missing value in the column.\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Fill the missing values with forward filling\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "print(df_ffill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e296a18-3bff-4fa0-ac37-79e828bb318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B   C\n",
      "0  1.0  5.0   9\n",
      "1  2.0  7.0  10\n",
      "2  4.0  7.0  11\n",
      "3  4.0  8.0  12\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Backward filling: In this method, the missing values are filled with the next non-missing value in the column.\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8], 'C': [9, 10, 11, 12]})\n",
    "\n",
    "# Fill the missing values with backward filling\n",
    "df_bfill = df.fillna(method='bfill')\n",
    "print(df_bfill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3c167f-4cc7-4117-940c-c07b9940cfe5",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled? <br>\n",
    "\n",
    "Imbalanced data refers to a situation where the distribution of classes in a dataset is not uniform, i.e., some classes have significantly more or fewer examples than others. For example, a dataset with 1000 examples, where 900 belong to class A and only 100 to class B, is an imbalanced dataset.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased models, where the model tends to predict the majority class more often than the minority class. In other words, the model may perform well on the majority class but poorly on the minority class. This is because the model's training is heavily influenced by the majority class, and it may not learn enough about the minority class.\n",
    "\n",
    "This can be a problem in many real-world scenarios, such as fraud detection, disease diagnosis, and anomaly detection, where the minority class is often more critical than the majority class.\n",
    "\n",
    "Therefore, it is essential to handle imbalanced data by using techniques such as:\n",
    "\n",
    "Undersampling: randomly removing examples from the majority class to balance the class distribution.\n",
    "Oversampling: creating synthetic examples for the minority class to balance the class distribution.\n",
    "SMOTE: generating synthetic examples for the minority class by interpolating between existing examples.\n",
    "Cost-sensitive learning: assigning different misclassification costs to different classes during training to make the model more sensitive to the minority class.\n",
    "By handling imbalanced data, we can improve the model's performance on the minority class, leading to more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e15c236-94be-4ab6-a76d-98c701cd8c70",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required. <br>\n",
    "\n",
    "Up-sampling and down-sampling are two techniques used to handle imbalanced datasets.\n",
    "\n",
    "Down-sampling involves reducing the number of examples in the majority class to balance the class distribution. This can be achieved by randomly selecting a subset of examples from the majority class equal to the number of examples in the minority class. For example, consider a binary classification problem where the positive class (minority) has only 10% of the examples, and the negative class (majority) has 90% of the examples. In this case, we can randomly select 10% of the negative examples to balance the class distribution.\n",
    "\n",
    "Up-sampling, on the other hand, involves increasing the number of examples in the minority class to balance the class distribution. This can be achieved by creating synthetic examples for the minority class using techniques such as SMOTE. For example, consider a binary classification problem where the positive class (minority) has only 10% of the examples, and the negative class (majority) has 90% of the examples. In this case, we can create synthetic examples for the positive class using SMOTE to balance the class distribution.\n",
    "\n",
    "When to use up-sampling and down-sampling?\n",
    "\n",
    "Down-sampling is typically used when we have a large number of examples in the majority class, and we can afford to discard some examples without significantly impacting the model's performance. Down-sampling is also useful when the majority class examples are very similar to each other, and we can still get good results by reducing their number.\n",
    "\n",
    "Up-sampling is used when we have a small number of examples in the minority class, and creating synthetic examples can help improve the model's performance. Up-sampling is also useful when the minority class examples are very diverse and hard to capture by the model, and creating synthetic examples can help the model learn better.\n",
    "\n",
    "In summary, down-sampling and up-sampling are techniques used to handle imbalanced datasets by balancing the class distribution. They are used when we have a large number of examples in the majority class or a small number of examples in the minority class, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f403e2-b765-41e3-a6de-0896b4142b60",
   "metadata": {},
   "source": [
    "Q5: What is data Augmentation? Explain SMOTE. <br>\n",
    "\n",
    "Data augmentation is a technique used to artificially increase the size of a dataset by creating new examples from the existing ones. This is done by applying transformations such as rotation, flipping, zooming, cropping, or adding noise to the original data.\n",
    "\n",
    "The purpose of data augmentation is to introduce variability in the dataset, making the model more robust and less sensitive to variations in the input data. Data augmentation is commonly used in computer vision and natural language processing tasks.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to address the problem of imbalanced datasets, where the minority class has fewer examples than the majority class. SMOTE generates synthetic examples for the minority class by interpolating between the existing examples.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "For each example in the minority class, SMOTE selects k nearest neighbors from the minority class (k is a user-defined parameter).\n",
    "\n",
    "SMOTE randomly selects one of the k neighbors and creates a new example by interpolating between the selected example and its neighbor. The interpolation is done as follows:\n",
    "\n",
    "For each feature of the example, SMOTE calculates the difference between the feature values of the selected example and its neighbor.\n",
    "\n",
    "SMOTE randomly selects a value between 0 and 1, and multiplies each feature difference by this value.\n",
    "\n",
    "SMOTE adds the resulting values to the feature values of the selected example to create a new synthetic example.\n",
    "\n",
    "SMOTE repeats this process until the desired number of synthetic examples is generated.\n",
    "\n",
    "The new synthetic examples generated by SMOTE are similar to the existing examples in the minority class but have small variations, making the model more robust and less sensitive to variations in the input data.\n",
    "\n",
    "In summary, SMOTE is a data augmentation technique used to generate synthetic examples for the minority class in an imbalanced dataset by interpolating between the existing examples. SMOTE helps address the problem of imbalanced datasets by making the model more robust and less sensitive to variations in the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3d33f-9883-49e4-8963-d45e0a88c86c",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers? <br>\n",
    "\n",
    "Outliers are data points that deviate significantly from the rest of the dataset. They can be caused by measurement errors, data entry errors, or represent rare but valid data points.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the analysis of the dataset and the performance of the machine learning models trained on it. Outliers can affect the mean and variance of the dataset, making it difficult to infer useful information from the data. Outliers can also skew the distribution of the data and affect the performance of machine learning models that assume a certain distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719d2c0-2bda-4e9e-a492-210f6494692a",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis? <br>\n",
    "\n",
    "There are several techniques that can be used to handle missing data in a dataset. Some of the commonly used techniques are:\n",
    "\n",
    "Deleting the missing data: This involves deleting the rows or columns that contain missing data. However, this approach should be used with caution as it can result in a loss of information and bias in the analysis.\n",
    "\n",
    "Imputing the missing data: This involves filling in the missing data with estimated values. There are several methods for imputing missing data, including mean imputation, mode imputation, regression imputation, and k-Nearest Neighbor (k-NN) imputation.\n",
    "\n",
    "Using a separate category: This involves creating a separate category or label for the missing data. This approach is commonly used in categorical variables.\n",
    "\n",
    "Using a predictive model: This involves using a predictive model to impute the missing data. The model is trained on the non-missing data and used to predict the missing values.\n",
    "\n",
    "Multiple imputation: This involves creating multiple imputed datasets, where the missing data is imputed multiple times using different methods. The analysis is then performed on each of the imputed datasets, and the results are combined using appropriate methods.\n",
    "\n",
    "In the context of analyzing customer data, the appropriate technique for handling missing data depends on the specific problem and the nature of the missing data. For example, if the missing data is a small percentage of the total data, imputing the missing data may be a good option. If the missing data is large or systematic, it may be better to consider a more complex technique such as multiple imputation or a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ea98c-e911-44cc-a398-91bb1782d402",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data? <br>\n",
    "\n",
    "When working with a large dataset, it is important to determine if the missing data is missing at random or if there is a pattern to the missing data. Here are some strategies to help identify the nature of the missing data:\n",
    "\n",
    "Look for patterns in missing data: One way to identify the nature of the missing data is to look for patterns in the missing data. For example, if the missing data is concentrated in certain variables or observations, it may indicate a non-random pattern to the missing data.\n",
    "\n",
    "Statistical tests: Another strategy to identify the nature of the missing data is to use statistical tests. One commonly used test is Little's test, which tests whether the missing data is missing completely at random (MCAR), missing at random (MAR), or not missing at random (NMAR). The results of this test can help determine the nature of the missing data.\n",
    "\n",
    "Visualizations: Visualization techniques can also be used to identify patterns in the missing data. For example, creating a heatmap of missing data can help identify if certain variables or observations have a higher percentage of missing data.\n",
    "\n",
    "Imputation methods: Finally, the imputation method used to handle missing data can also provide insights into the nature of the missing data. For example, if mean imputation results in significantly different results than other imputation methods, it may indicate that the missing data is not missing completely at random.\n",
    "\n",
    "In summary, strategies such as looking for patterns, statistical tests, visualizations, and imputation methods can be used to identify if the missing data is missing at random or if there is a pattern to the missing data. Understanding the nature of the missing data is essential for selecting an appropriate imputation method and ensuring accurate analysis of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ca523-c4cf-4f7d-ac02-c52340d27218",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset? <br>\n",
    "\n",
    "Working with imbalanced datasets is a common challenge in machine learning, especially in medical diagnosis projects where the condition of interest may be rare. Here are some strategies you can use to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "\n",
    "Use evaluation metrics that are suitable for imbalanced datasets: Accuracy is not an appropriate metric for imbalanced datasets because it does not take into account the class imbalance. Instead, metrics such as precision, recall, F1-score, and Area Under the Receiver Operating Characteristic Curve (AUROC) are more appropriate for imbalanced datasets.\n",
    "\n",
    "Use resampling techniques: Resampling techniques such as oversampling the minority class, undersampling the majority class, or using a combination of both can help balance the dataset and improve the performance of the machine learning model. Oversampling techniques such as Synthetic Minority Over-sampling Technique (SMOTE) can be used to generate synthetic data points to balance the dataset.\n",
    "\n",
    "Use cost-sensitive learning: In cost-sensitive learning, different weights are assigned to the different classes to reflect their importance. For example, misclassifying a positive case as negative may have more severe consequences than misclassifying a negative case as positive.\n",
    "\n",
    "Use ensemble techniques: Ensemble techniques such as bagging, boosting, and stacking can be used to improve the performance of the machine learning model on imbalanced datasets by combining multiple models.\n",
    "\n",
    "Use cross-validation: Cross-validation can be used to ensure that the model is not overfitting to the training data and to estimate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44050773-4c88-4a91-944b-5ba6b51e9cca",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class? <br>\n",
    "\n",
    "To balance an unbalanced dataset where the majority class is overrepresented, one common technique is to down-sample the majority class. Here are some methods that can be employed to down-sample the majority class:\n",
    "\n",
    "Random under-sampling: In this technique, a random sample of the majority class is taken so that the number of samples in the majority class is reduced to the same number of samples as in the minority class. This approach is simple but may result in the loss of some useful information.\n",
    "\n",
    "Cluster centroids: In this technique, the majority class is clustered, and the centroids of each cluster are used as representative samples of the majority class. This approach helps preserve the distribution of the majority class and can be useful when there is significant overlap between the classes.\n",
    "\n",
    "Tomek links: Tomek links are pairs of data points from different classes that are close to each other. In this technique, the majority class samples that are Tomek links with the minority class are removed. This approach helps improve the margin between the classes and can be used in combination with other techniques.\n",
    "\n",
    "NearMiss: Near-miss is an algorithm that can help in balancing an imbalanced dataset. It can be grouped under undersampling algorithms and is an efficient way to balance the data. The algorithm does this by looking at the class distribution and randomly eliminating samples from the larger class. When two points belonging to different classes are very close to each other in the distribution, this algorithm eliminates the datapoint of the larger class thereby trying to balance the distribution. \n",
    "\n",
    "Edited nearest neighbors: In this technique, the majority class samples that are classified incorrectly by their nearest neighbors from the minority class are removed. This approach helps remove noisy samples from the majority class and can be useful when there is significant overlap between the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7dc32a-2fc8-4848-80b5-a82363fb481c",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?<br>\n",
    "\n",
    "To balance an unbalanced dataset with a low percentage of occurrences, one approach is to up-sample the minority class. Here are some methods that can be employed to up-sample the minority class:\n",
    "\n",
    "Random over-sampling: This method involves randomly duplicating observations from the minority class to increase its size. This method is simple but can lead to overfitting if not carefully applied.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): This method involves creating synthetic samples for the minority class by interpolating between existing minority class samples. This method can be effective in increasing the size of the minority class without overfitting.\n",
    "\n",
    "ADASYN (Adaptive Synthetic Sampling): This method is similar to SMOTE but generates synthetic samples based on the density distribution of the minority class. This method can be effective in generating more diverse synthetic samples than SMOTE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
