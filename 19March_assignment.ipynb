{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b725577-b283-4dd2-a345-bf88faddfd46",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application. <br>\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique that rescales the values of a feature to a fixed range between 0 and 1. It is often used to bring all features to the same scale to avoid bias towards one feature over another in machine learning algorithms that are sensitive to the magnitude of the input data.\n",
    "\n",
    "The formula for Min-Max scaling is as follows: <br>\n",
    "X_normalized = (X - X_min) / (X_max - X_min) <br>\n",
    "where X is the original value of the feature, X_min is the minimum value of the feature, and X_max is the maximum value of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dece2c-634b-4676-bb4e-fb008d902603",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.<br>\n",
    "\n",
    "Unit vector scaling rescales the feature vector (i.e., the collection of all feature values for a particular data point) to have a length of 1 while preserving the direction of the vector. Each individual feature value in the vector is divided by the length of the vector, which is calculated using the Euclidean norm, and in this way, the length of the feature vector is rescaled to 1. <br>\n",
    "\n",
    "X_normalized = X / ||X|| <br>\n",
    "In contrast to Min-Max scaling, which rescales the values of a feature to a fixed range, unit vector scaling only rescales the length of the feature vector, while keeping its direction intact. This means that the values of the feature can have varying ranges after scaling.\n",
    "\n",
    "For example, consider a dataset with two features: \"height\" and \"weight\". We can combine these features into a single vector, X = [height, weight], and normalize it using the unit vector scaling technique.\n",
    "\n",
    "Let's say we have a data point with a height of 170 cm and a weight of 65 kg. Then, we can compute its normalized vector as follows:<br>\n",
    "||X|| = sqrt(170^2 + 65^2) = 182.14 <br>\n",
    "X_normalized = [170/182.14, 65/182.14] = [0.934, 0.357] <br>\n",
    "\n",
    "Thus, the normalized vector of this data point's \"height\" and \"weight\" features is [0.934, 0.357]. We can apply the same formula to all other data points in the dataset to normalize the entire feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed266a4-f493-48e6-8995-ae7098c24c16",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application. <br>\n",
    "\n",
    "PCA, or Principal Component Analysis, is a data analysis technique used to identify patterns in high-dimensional data by reducing its dimensionality. It works by identifying a new set of variables, called principal components, which are linear combinations of the original variables that explain the maximum amount of variance in the data.\n",
    "\n",
    "The basic steps involved in PCA are as follows:\n",
    "\n",
    "Standardize the data: All the features are centered and scaled to have a mean of zero and a variance of one.\n",
    "\n",
    "Compute the covariance matrix: The covariance matrix is calculated to determine the strength of the relationship between pairs of variables.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues of the covariance matrix are calculated to determine the directions of the principal components and their importance in explaining the variance in the data.\n",
    "\n",
    "Select the principal components: The principal components are selected based on the magnitude of their corresponding eigenvalues. The first principal component has the largest eigenvalue and explains the maximum amount of variance, followed by the second principal component, and so on.\n",
    "\n",
    "Project the data onto the new feature space: The original data is transformed onto the new feature space defined by the selected principal components.\n",
    "\n",
    "PCA is commonly used in dimensionality reduction to reduce the number of features in a dataset while retaining most of the information. It does this by selecting the principal components that explain the majority of the variance in the data and discarding the rest.\n",
    "\n",
    "For example, consider a dataset with three features: \"age\", \"income\", and \"education level\". We can apply PCA to this dataset to identify the principal components that explain the maximum amount of variance. Let's say that after PCA, we find that the first principal component is a linear combination of all three features that explains 80% of the variance in the data. This suggests that the three features are highly correlated and can be represented by a single principal component.\n",
    "\n",
    "We can then use this principal component as a new feature in our analysis, effectively reducing the dimensionality of the dataset from three to one. This can improve the efficiency of our machine learning algorithms while retaining most of the information in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ed9a3-d91f-411c-b332-e7933f814b7e",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.<br>\n",
    "\n",
    "PCA and feature extraction are closely related concepts in machine learning, as PCA is a common technique used for feature extraction.\n",
    "\n",
    "Feature extraction involves transforming the original features of a dataset into a new set of features that are more informative and discriminative for a particular task. This can involve combining or reducing the original features in some way to capture the underlying structure of the data more effectively.\n",
    "\n",
    "PCA is a technique that can be used for feature extraction by identifying the principal components of a dataset and using them as new features. These principal components are linear combinations of the original features that capture the maximum amount of variance in the data, and can be thought of as representing the most important underlying patterns or structures in the data.\n",
    "\n",
    "To illustrate this concept, consider a dataset of images of handwritten digits. Each image is represented as a high-dimensional vector of pixel values. We can use PCA to extract features from these images by identifying the principal components that capture the most important patterns in the images.\n",
    "\n",
    "Specifically, we can apply PCA to the pixel values of the images and identify the principal components that explain the maximum amount of variance in the data. These principal components can then be used as new features for the images, replacing the original pixel values.\n",
    "\n",
    "For example, the first principal component might correspond to the overall darkness of the image, while the second principal component might correspond to the slant of the digit. By using these principal components as new features, we can reduce the dimensionality of the dataset and capture the most important patterns in the images in a more compact and informative way.\n",
    "\n",
    "We can then use these new features as input to a machine learning algorithm, such as a neural network, to classify the images into their corresponding digit categories (0-9). By using PCA for feature extraction, we can improve the accuracy and efficiency of the classification task by reducing the number of input features and capturing the most important patterns in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58bc145-5b67-4ab0-ba16-acf9eef574b3",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data. <br>\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to rescale numerical features to a fixed range, typically between 0 and 1. In the context of building a recommendation system for a food delivery service, we can use Min-Max scaling to preprocess the features in the dataset, such as price, rating, and delivery time, before applying machine learning algorithms to the data.\n",
    "\n",
    "The first step in using Min-Max scaling is to compute the minimum and maximum values of each feature in the dataset. For example, we can calculate the minimum and maximum price, rating, and delivery time in the dataset.\n",
    "\n",
    "Once we have the minimum and maximum values for each feature, we can use the following formula to rescale each value to a range between 0 and 1:\n",
    "\n",
    "scaled_value = (original_value - minimum_value) / (maximum_value - minimum_value)\n",
    "\n",
    "For example, if the minimum price in the dataset is $5 and the maximum price is $20, we can rescale the price of each food item using the formula above to obtain a value between 0 and 1. Similarly, we can rescale the ratings and delivery times to a range between 0 and 1 using their respective minimum and maximum values.\n",
    "\n",
    "After rescaling the features using Min-Max scaling, we can then use the scaled features as input to our recommendation system algorithm, such as collaborative filtering or matrix factorization, to generate personalized food recommendations for each user.\n",
    "\n",
    "Overall, using Min-Max scaling to preprocess the features in our dataset can help us to normalize the data and ensure that each feature is on the same scale, which can improve the performance of our recommendation system and help us to make more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077a6aac-6973-4f68-97a0-b49a1dcb3af4",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset. <br>\n",
    "\n",
    "In the context of building a model to predict stock prices, the dataset may contain many features that are highly correlated with each other, making it difficult to build an accurate model. This is where PCA can be used to reduce the dimensionality of the dataset by identifying the most important underlying patterns in the data and representing them using a smaller set of principal components.\n",
    "\n",
    "To use PCA to reduce the dimensionality of the dataset, we first need to preprocess the data by standardizing the features to have a mean of 0 and a standard deviation of 1. This is important because PCA is sensitive to the scale of the features and can give misleading results if the features are not standardized.\n",
    "\n",
    "Once the features have been standardized, we can apply PCA to the data to identify the principal components that capture the most important patterns in the data. PCA works by identifying the directions in which the data varies the most, and projecting the data onto a lower-dimensional subspace spanned by these directions.\n",
    "\n",
    "The number of principal components we choose to keep in the reduced-dimensional dataset will depend on the amount of variance we want to retain in the data. We can use a scree plot or cumulative explained variance plot to help us decide how many principal components to keep.\n",
    "\n",
    "Once we have identified the principal components to keep, we can use them as new features in our model to predict stock prices. These new features will be linear combinations of the original features, and will capture the most important underlying patterns in the data.\n",
    "\n",
    "For example, the first principal component might represent overall market trends, while the second principal component might represent financial performance of the company. By using these principal components as new features in our model, we can reduce the dimensionality of the dataset and improve the accuracy and efficiency of our stock price prediction model.\n",
    "\n",
    "Overall, using PCA to reduce the dimensionality of the dataset can help us to identify the most important underlying patterns in the data and reduce the number of features we need to consider in our model, which can improve the performance of our stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd19db2-ea74-4cd9-b981-e2d721238a25",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28d44bc-2c51-4616-8adc-519c0749f13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lst =[[1],[5],[10],[15],[20]]\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max = MinMaxScaler()\n",
    "min_max.fit_transform(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc04a80-9e80-4e58-8555-59956513bbef",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why? <br>\n",
    "\n",
    "\n",
    "The number of principal components to retain in PCA depends on various factors such as the amount of variance explained by each component, the desired level of dimensionality reduction, and the trade-off between the amount of information retained and the complexity of the model.\n",
    "\n",
    "To determine the number of principal components to retain, one common approach is to look at the cumulative explained variance as a function of the number of components. The cumulative explained variance tells us the percentage of total variance in the data that is explained by the first k principal components. We can then choose the number of components that capture a sufficiently high percentage of the total variance, while also keeping the model simple.\n",
    "\n",
    "In practice, a common rule of thumb is to retain enough principal components to explain at least 70-80% of the total variance. However, the actual number of components may vary depending on the specific dataset and the problem at hand.\n",
    "\n",
    "For the given dataset with features of height, weight, age, gender, and blood pressure, we would first need to preprocess the data by standardizing or normalizing each feature. Then, we can apply PCA to extract the principal components and calculate the explained variance.\n",
    "\n",
    "Once we have the explained variance for each component, we can plot the cumulative explained variance as a function of the number of components and choose the number of components that capture at least 70-80% of the total variance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
